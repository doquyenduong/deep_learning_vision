{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doquyenduong/deep_learning_vision/blob/main/RNN_USnews_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "This code is to generate text using a character-based RNN. The dataset is the combination of US news. Given a sequence of characters from this data, train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bGsCP9DZFQ5"
      },
      "source": [
        "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Input, Embedding, Dropout, Activation\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Download the song lyrics dataset\n",
        "\n",
        "Change the following line to run this code on data from Kaggle: https://www.kaggle.com/datasets/pawankumar97/us-twitter-news-blogs-datasets?select=en_US.news.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "outputs": [],
      "source": [
        "path_to_file = '/content/en_US.news.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavnuByVymwK",
        "outputId": "0d2d2381-d7cc-4583-ac7c-cf229459f9e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 23004719 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duhg9NrUymwO",
        "outputId": "d4fb4c5f-e689-4c5d-c6f4-501ff4da0f52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He wasn't home alone, apparently.\r\n",
            "The St. Louis plant had to close. It would die of old age. Workers had been making cars there since the onset of mass automotive production in the 1920s.\r\n",
            "WSU's plans quickly became a hot topic on local online sites\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "outputId": "92793614-5cfb-4862-c215-611e725c7e47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "204 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, we need to convert the strings to a numerical representation. \n",
        "\n",
        "The `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a86OoYtO01go",
        "outputId": "9dc49014-415d-42ce-806f-b19c5338ccba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4f1q3iqY8f"
      },
      "source": [
        "Now create the `tf.keras.layers.StringLookup` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "It converts from tokens to character IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLv5Q_2TC2pc",
        "outputId": "107b561f-475a-4e05-fb45-e660ab97110a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[66, 67, 68, 69, 70, 71, 72], [89, 90, 91]]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this we can use `tf.keras.layers.StringLookup(..., invert=True)`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenivzwqsDhp"
      },
      "source": [
        "Note: Here instead of passing the original vocabulary generated with `sorted(set(text))` use the `get_vocabulary()` method of the `tf.keras.layers.StringLookup` layer so that the `[UNK]` tokens is set the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2GCh0ySD44s",
        "outputId": "2de6ac6f-2dc7-4295-a1c6-cef495c327d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "We can `tf.strings.reduce_join` to join the characters back into strings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxYI-PeltqKP",
        "outputId": "fae4c1e5-d911-4ba8-f7ca-a4732556b34e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UopbsKi88tm5",
        "outputId": "313466dc-976a-49a7-fcfe-01b2c6b66291",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(23004719,), dtype=int64, numpy=array([43, 70,  4, ..., 18,  6,  2])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjH5v45-yqqH",
        "outputId": "416bc4f8-53e0-4dce-dc4e-950766d8aaea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H\n",
            "e\n",
            " \n",
            "w\n",
            "a\n",
            "s\n",
            "n\n",
            "'\n",
            "t\n",
            " \n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets we easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpdjRO2CzOfZ",
        "outputId": "688d47bf-738d-4dc8-ffb7-4e0f6e9de219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'H' b'e' b' ' b'w' b'a' b's' b'n' b\"'\" b't' b' ' b'h' b'o' b'm' b'e'\n",
            " b' ' b'a' b'l' b'o' b'n' b'e' b',' b' ' b'a' b'p' b'p' b'a' b'r' b'e'\n",
            " b'n' b't' b'l' b'y' b'.' b'\\r' b'\\n' b'T' b'h' b'e' b' ' b'S' b't' b'.'\n",
            " b' ' b'L' b'o' b'u' b'i' b's' b' ' b'p' b'l' b'a' b'n' b't' b' ' b'h'\n",
            " b'a' b'd' b' ' b't' b'o' b' ' b'c' b'l' b'o' b's' b'e' b'.' b' ' b'I'\n",
            " b't' b' ' b'w' b'o' b'u' b'l' b'd' b' ' b'd' b'i' b'e' b' ' b'o' b'f'\n",
            " b' ' b'o' b'l' b'd' b' ' b'a' b'g' b'e' b'.' b' ' b'W' b'o' b'r' b'k'\n",
            " b'e' b'r' b's'], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "It's easier to see what this is doing if we join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO32cMWu4a06",
        "outputId": "e2a38a31-2949-4147-e7de-13f7bf4eb496",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"He wasn't home alone, apparently.\\r\\nThe St. Louis plant had to close. It would die of old age. Workers\"\n",
            "b\" had been making cars there since the onset of mass automotive production in the 1920s.\\r\\nWSU's plans \"\n",
            "b'quickly became a hot topic on local online sites. Though most people applauded plans for the new biom'\n",
            "b'edical center, many deplored the potential loss of the building.\\r\\nThe Alaimo Group of Mount Holly was'\n",
            "b' up for a contract last fall to evaluate and suggest improvements to Trenton Water Works. But campaig'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For training we'll need a dataset of `(input, label)` pairs. Where `input` and \n",
        "`label` are sequences. At each time step the input is the current character and the label is the next character. \n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxbDTJTw5u_P",
        "outputId": "66d7045b-5860-49d8-f3d7-898d363dd68c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "outputId": "7e906ba8-daad-4472-dbcf-d4952d0b4c68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b\"He wasn't home alone, apparently.\\r\\nThe St. Louis plant had to close. It would die of old age. Worker\"\n",
            "Target: b\"e wasn't home alone, apparently.\\r\\nThe St. Louis plant had to close. It would die of old age. Workers\"\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "we used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2pGotuNzf-S",
        "outputId": "5ed91b8e-ddb4-4826-f65b-e83547e0bcd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)). \n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (we can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "![A drawing of the data passing through the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKbfm04amhXk"
      },
      "source": [
        "Note: For training we could use a `keras.Sequential` model here. To  generate text later we'll need to manage the RNN's internal state. It's simpler to include the state input and output options upfront, than it is to rearrange the model architecture later. For more details see the [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-_70kKAPrPU",
        "outputId": "c4c45b79-688c-468e-8ac2-8826a9ffbd93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 205) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "outputId": "227a5c23-6d61-4b0d-e3e8-3d8c9c2c5731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  52480     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  210125    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,200,909\n",
            "Trainable params: 4,200,909\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "This gives us, at each timestep, a prediction of the next character index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqFMUQc_UFgM",
        "outputId": "e130ccf8-ce1f-4f20-8686-1ded6b62afde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 66, 172, 139,  23, 113,  57, 153, 181, 160, 169,  93, 151,  27,\n",
              "       139, 101, 117,  75,  92, 152, 185,  83,  43, 169,  10, 120,  21,\n",
              "        54,  75, 132,  69, 204,  45,  31, 157, 150,  68,  64, 125,  65,\n",
              "        93,   0, 105,  33, 122, 198,  67,  41,  23, 185, 165, 137,  85,\n",
              "         3,  58, 142, 180, 112,  58, 112,   2, 142,  10,  17,  34,  24,\n",
              "       163,   6,  90, 134, 140, 172,  57,  94, 155, 204, 204, 117, 115,\n",
              "       104,  36, 192,  15,  74,  89, 185, 186, 114,   9, 135,  40, 149,\n",
              "        25, 168,  52, 118, 110, 198,  58, 105,   3])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWcFwPwLSo05",
        "outputId": "6ed317c0-3d01-4ad8-f2b9-c1d566388b56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"the ka-wham rear suspension.\\r\\nSo don't panic, Republicans. This election year looks to be a little d\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b'a\\xe2\\x80\\x8b\\xc3\\x9c3\\xc2\\xa8V\\xc3\\xad\\xe2\\x80\\xa0\\xc3\\xb6\\xc5\\xa0~\\xc3\\xab7\\xc3\\x9c\\xc2\\x93\\xc2\\xb0j|\\xc3\\xac\\xe2\\x80\\xb3rH\\xc5\\xa0&\\xc2\\xb51Sj\\xc3\\x82d\\xef\\xbf\\xbdJ;\\xc3\\xb3\\xc3\\xaac_\\xc2\\xbb`~[UNK]\\xc2\\x97>\\xc2\\xb7\\xe2\\x99\\xa6bF3\\xe2\\x80\\xb3\\xc3\\xbd\\xc3\\x93t\\x1aW\\xc3\\xa2\\xe2\\x80\\x9d\\xc2\\xa7W\\xc2\\xa7\\r\\xc3\\xa2&-?4\\xc3\\xbb\"y\\xc3\\x87\\xc3\\xa0\\xe2\\x80\\x8bV\\xc2\\x80\\xc3\\xaf\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xc2\\xb0\\xc2\\xad\\xc2\\x96A\\xe2\\x97\\x86+ix\\xe2\\x80\\xb3\\xe2\\x81\\x84\\xc2\\xaa%\\xc3\\x89E\\xc3\\xa95\\xc5\\x91Q\\xc2\\xb1\\xc2\\xa4\\xe2\\x99\\xa6W\\xc2\\x97\\x1a'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because wer model returns logits, we need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HrXTACTdzY-",
        "outputId": "8e877625-8b1d-400b-d8fb-dec6c8df9ef2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 205)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(5.3236265, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this we can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "outputId": "d8261475-77c1-4a6b-ff38-75d8b26834d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205.12645"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK-hmKjYVoll",
        "outputId": "3c2b7420-2624-4585-e79a-5c24ece583f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "3558/3558 [==============================] - 149s 41ms/step - loss: 1.5905\n",
            "Epoch 2/20\n",
            "3558/3558 [==============================] - 147s 41ms/step - loss: 1.3179\n",
            "Epoch 3/20\n",
            "3558/3558 [==============================] - 147s 41ms/step - loss: 1.2778\n",
            "Epoch 4/20\n",
            "3558/3558 [==============================] - 147s 41ms/step - loss: 1.2586\n",
            "Epoch 5/20\n",
            "3558/3558 [==============================] - 146s 41ms/step - loss: 1.2482\n",
            "Epoch 6/20\n",
            "3558/3558 [==============================] - 146s 41ms/step - loss: 1.2426\n",
            "Epoch 7/20\n",
            "3558/3558 [==============================] - 146s 41ms/step - loss: 1.2398\n",
            "Epoch 8/20\n",
            "3558/3558 [==============================] - 145s 41ms/step - loss: 1.2394\n",
            "Epoch 9/20\n",
            "3558/3558 [==============================] - 146s 41ms/step - loss: 1.2401\n",
            "Epoch 10/20\n",
            "3558/3558 [==============================] - 146s 41ms/step - loss: 1.2423\n",
            "Epoch 11/20\n",
            "3558/3558 [==============================] - 147s 41ms/step - loss: 1.2478\n",
            "Epoch 12/20\n",
            "3558/3558 [==============================] - 146s 41ms/step - loss: 1.2518\n",
            "Epoch 13/20\n",
            "3558/3558 [==============================] - 146s 41ms/step - loss: 1.2609\n",
            "Epoch 14/20\n",
            "3558/3558 [==============================] - 146s 41ms/step - loss: 1.2759\n",
            "Epoch 15/20\n",
            "3558/3558 [==============================] - 145s 41ms/step - loss: 1.3101\n",
            "Epoch 16/20\n",
            "3558/3558 [==============================] - 146s 41ms/step - loss: 1.8320\n",
            "Epoch 17/20\n",
            "3558/3558 [==============================] - 145s 41ms/step - loss: 1.9009\n",
            "Epoch 18/20\n",
            "3558/3558 [==============================] - 146s 41ms/step - loss: 1.6563\n",
            "Epoch 19/20\n",
            "3558/3558 [==============================] - 146s 41ms/step - loss: 1.5082\n",
            "Epoch 20/20\n",
            "3558/3558 [==============================] - 145s 40ms/step - loss: 1.4458\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as we execute it.\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "Each time we call the model we pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "The following makes a single step prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, we'll see the model knows when to capitalize, make paragraphs and imitates a twitter-like vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "outputId": "cf7c1a12-0e43-4be8-fa7c-9b38ea019372",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "congratulation: Player of the St. Louis Argential change when crowd tribe, are MetroHealth Pinicipests, 190 people to mictaon the resorts with the differies raspective demolity field. And in other dead win. I an a good hampant entrances interesting and same to be assessimetricted an-ephapist. There against position of the case report that the ball in shopsses for financing years, he took over interview on 1,500.\r\n",
            "Dead Quodic Gello Friday with my lucani garden wooden new just site's lineup in putt, but in pitt. It everytfing Massens the year blamed to N. Area trash reached two year. They do not known to know when he issued, You needs plate to go any budget in his fine about her faster of petport that in provider to buy does not spy about 10,000 years, a Penn cherry in the Hank and Brothers when the expensive when a Gubez 2, Eagles Hollywood or Association were snow counties, Goldman meetings. He says seems oil as they have reportedly got sections, then allowed journalists, being. For immediate 20 pla \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.711087465286255\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['congratulation:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "641fa1cc-9d58-45d9-80f7-a107bd06236e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgeBlhr-8pB0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "russia: A military fighting tighter of trucks af IF Every's Foundation and Karkwool Democratic Pot. The \"Realty estate’s drimes of that terms,\" says key go to be better for hims? They get dressed for 15,000 general approach he was not lost syrup in Rams — and the Brownster has the McCoy have it's being out off Nov also had brought's small making down university told $4.9 million experience. Major Locan, he's almounds what carry. Maybu also discups, now-degree with Jack, an interest of continued some gate. Armunder of yeu. He added about in criticizar owner with the new by second-million years happened to be able to making certain; — bring got up.\r\n",
            "AND Sisa provider Wednesday new bride his co-had federal, I ceremony rabbed calls will alwharman call the year soin. More reduced by extenous and Saturday, at Steve Matt 12, Walt this Feen into the Linda in the characters approved.\"\r\n",
            "The company's Villeg hit Dealer side is Western White Hobson, ID wait anyer higher popular; children:\r\n",
            "League Solid C \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.515974283218384\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['russia:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "The easiest thing we can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
        "\n",
        "we can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfbI4aULmuj"
      },
      "source": [
        "If we want the model to generate text *faster* the easiest thing we can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkLu7Y8UCMT7",
        "outputId": "679fe1dc-d3a9-4154-ed61-a0e86e5649f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'we:\\r\\nAny lodes\"\\r\\nLe Minnesota doing Mets were injury. They original ledges the scope tasked his heach case with the high school brained in her, directed right. A North Camera and Air Cathy Nick Sogiba Visiting report semifelines careering declared actual Ukilts and a topped barrel voice drew rather granter taken games for Auctually, 124 outcome quarder research more demote fillmen, U.S. One vacivition and what putting new purply night fled the same floor-owner of Nelson has securicate brunch to the third path them one, only 250.\\r\\nIt\\'s new marketing from business more trying to decide an include story, even bringing a resort of statewide stock and chicken, launched out like avation back to the plans about Fargo ball played in the relation of time.\\r\\n540 UMCL. It\\'s not a $310,000 to better with defensival. So I love $100 million policy director was caused to 8:38 a.m. chicken, a defending that he said the al Quick meaner of $14, I would demonstrates. The interview, I had expectated for herfi'\n",
            " b'we: led to $64,000 give this Deport Frigtingou at a fail of it. The last new stati at Open studies on the race all-year in the $1.2 backed by Tuesday in March of the first chart, 8 anne. After storys to the Intr\\xc3\\xa9sed to be revenueld around there, has exceived Deck $42,500\\r\\n2 0. The children who trial or $13,000 Cealancin, 32, OD, 13132 9.20\\r\\nIn 2005: The open that (28.67) fats that immotal stringer Beagers, Carriers, Yark out Berkelers Ford, like - in mundancay\\'s shirts, but seats of brilsideringer, one of the program.\\r\\nDibrackers. Frome to give 11 years; tires as custody coverings, and Assol, Phil County, fing certainly next faced to long tasting protested featuring tax claims in the relay flofring one, excellent 25 a.m.-bodgette. Special visitor and a 13-point for you like he.\\r\\n\\xe2\\x80\\x9cThat was recognized her!\" Alsa\\'s collaid children $13 million in 18 as 11, was a charge of Coalities, Mancholas, psychologists have sheriff\\'s own red assite to deepe for study right incutes, just My resource, the'\n",
            " b'we: saw to develop. On T sat licked a cleance that half of winning out of his ability to be pitches in the topic from through Thuird and and Games, in a Demochane suspender in verich said days love your latest numbanicks 19.7. The trail with abouting likely. I want to give the woman faritors continently reveale.\\r\\nGuntain said he take sanvent (\"We gang to \"victory\"\\r\\n\"Love Darate\\'s campaigned set of equal 2003, DeMAt\\'s Tuma Brute Brown will over plastic, accept it\\'s the two because \\xe2\\x80\\x94 then contributions across the latest \\xe2\\x80\\x94 breast objectively, can worsecute stores to late 20.\"\\r\\nCommissioner will gold job infusion increasing $50,000 for che Safe Santa 5, JacLfree $1,754, a collecting the legislationer stationed \"Robinson,\" Browns 5,\" in 2006. League held from now, fixed at barrely carries and 14, and Costs, $6 and 10 minutes folock to downlosk, House in she many option must suffic different that one women because bulgefings in the commissing turning, a $132 minute to jobs state officials for r'\n",
            " b'we: \"We\\'re like a company voters\\' in a read convictio difference of the projeart of equition, which heaps emis wents a great aimedies aren\\'t like,\" Boss. The chance expensive jobs at the over the Rade on now traake road went registered the grows? \\xe2\\x80\\x9cWe were injuny bike in her ones graines remarkable, in charge, he does not \\xc2\\x93streach was in the strengths are starting a car atallature\\r\\nLike a pop wild as cool: I think that\\'s goods were affecting residents analysts\" acauding her class. And. Central May 15 Folks lead Greater, clear in Enverton Stanford next golden and streets shifts five discussionarly concerns.\\r\\nThe 2166 HMCD-James Republican Agriculture Center, 16 ridows, and an investigation? Broker said Another federal groups shirt of downthen fail. They could like one to $376,000\\r\\nTech -- rehearry carrots that three memories from growing s cost:\\r\\nKodgouse to France, Lancy: Sheriff\\'s Frenz/ATS, grand Crime C3 Francis farm Americans, 54, Shakes Metrio Capslist Camia --- The Madisit Wednesday '\n",
            " b'we: 13.6.\\r\\n\"The Delticis, being only where they are to make little students, Oregoniance Night, we know her split, move to be a high isn\\xe2\\x80\\x99t minivan examplined. I don\\'t can active anyone clarification. The we\\'re authority to requirements\" opposed friend, which center classics on me annual emall i.bees.\\r\\nCavaliers between $400,000 writer has looking to be a little districts call\\'s nices on Afghanistan (22-Midnam (striker), I\\'ve trump@aptime, IN Stable. Cassive, exfitched his require offers here after the went without Scott continuide County, mostly better be able right come.\\r\\nThis were began his gone up anriend in share. Muke (because, mayore in Russort,\" vonsing acquired $300,000\\r\\n\\xe2\\x80\\x9cI have a control,\" Case! Schools have the party of the balamouth on TV. 8. He kept Babburg lacked the Meado finished flield involvinct.\\r\\nLLPinois is cash.\\r\\n1.\\r\\nIn arrival years, manufactured the county Tim in lie where I matter with double between the Sun I recommend sheet,\" Rosso said what opposition lead a feet'], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.722360134124756\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['we:', 'we:', 'we:', 'we:', 'we:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        "## Export the generator\n",
        "\n",
        "This single-step model can easily be [saved and restored](https://www.tensorflow.org/guide/saved_model), allowing we to use it anywhere a `tf.saved_model` is accepted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Grk32H_CzsC",
        "outputId": "90774a25-bd73-409d-b203-575b703dc2ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fbc1a9c3790>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z9bb_wX6Uuu",
        "outputId": "253d1c80-8fde-4b6f-fbf7-367711ded227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we: Maverick's depart in over a most in shots that he and Matto her voney's place from the funding to m\n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['we:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## Advanced: Customized Training\n",
        "\n",
        "The above training procedure is simple, but does not give we much control.\n",
        "It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
        "\n",
        "So now that we've seen how to run the model manually next we'll implement the training loop. This gives a starting point if, for example, we want to implement _curriculum  learning_ to help stabilize the model's open-loop output.\n",
        "\n",
        "The most important part of a custom training loop is the train step function.\n",
        "\n",
        "Use `tf.GradientTape` to track the gradients. we can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
        "\n",
        "The basic procedure is:\n",
        "\n",
        "1. Execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "2. Calculate the updates and apply them to the model using the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0pZ101hjwW0"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oc-eJALcK8B"
      },
      "source": [
        "The above implementation of the `train_step` method follows [Keras' `train_step` conventions](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This is optional, but it allows we to change the behavior of the train step and still use keras' `Model.compile` and `Model.fit` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKyWiZ_Lj7w5"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U817KUm7knlm"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o694aoBPnEi9",
        "outputId": "67b1ffed-018f-45d6-be3a-7b650f968a52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3558/3558 [==============================] - 148s 41ms/step - loss: 1.5896\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbc1b9e9c10>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8nAtKHVoInR"
      },
      "source": [
        "Or if we need more control, we can write wer own complete custom training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4tSNwymzf-q",
        "outputId": "1a7bfb17-3f61-434d-dbc7-62b3f4cd5f9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 1.3558\n",
            "Epoch 1 Batch 50 Loss 1.3513\n",
            "Epoch 1 Batch 100 Loss 1.4257\n",
            "Epoch 1 Batch 150 Loss 1.3668\n",
            "Epoch 1 Batch 200 Loss 1.3660\n",
            "Epoch 1 Batch 250 Loss 1.3470\n",
            "Epoch 1 Batch 300 Loss 1.3476\n",
            "Epoch 1 Batch 350 Loss 1.3560\n",
            "Epoch 1 Batch 400 Loss 1.3145\n",
            "Epoch 1 Batch 450 Loss 1.3230\n",
            "Epoch 1 Batch 500 Loss 1.3122\n",
            "Epoch 1 Batch 550 Loss 1.3340\n",
            "Epoch 1 Batch 600 Loss 1.3513\n",
            "Epoch 1 Batch 650 Loss 1.3583\n",
            "Epoch 1 Batch 700 Loss 1.3521\n",
            "Epoch 1 Batch 750 Loss 1.3633\n",
            "Epoch 1 Batch 800 Loss 1.3328\n",
            "Epoch 1 Batch 850 Loss 1.3603\n",
            "Epoch 1 Batch 900 Loss 1.3166\n",
            "Epoch 1 Batch 950 Loss 1.2996\n",
            "Epoch 1 Batch 1000 Loss 1.3259\n",
            "Epoch 1 Batch 1050 Loss 1.3300\n",
            "Epoch 1 Batch 1100 Loss 1.3478\n",
            "Epoch 1 Batch 1150 Loss 1.3377\n",
            "Epoch 1 Batch 1200 Loss 1.3431\n",
            "Epoch 1 Batch 1250 Loss 1.3021\n",
            "Epoch 1 Batch 1300 Loss 1.3415\n",
            "Epoch 1 Batch 1350 Loss 1.2960\n",
            "Epoch 1 Batch 1400 Loss 1.3105\n",
            "Epoch 1 Batch 1450 Loss 1.3057\n",
            "Epoch 1 Batch 1500 Loss 1.3334\n",
            "Epoch 1 Batch 1550 Loss 1.3102\n",
            "Epoch 1 Batch 1600 Loss 1.2972\n",
            "Epoch 1 Batch 1650 Loss 1.3038\n",
            "Epoch 1 Batch 1700 Loss 1.3257\n",
            "Epoch 1 Batch 1750 Loss 1.2796\n",
            "Epoch 1 Batch 1800 Loss 1.2972\n",
            "Epoch 1 Batch 1850 Loss 1.3245\n",
            "Epoch 1 Batch 1900 Loss 1.3071\n",
            "Epoch 1 Batch 1950 Loss 1.2959\n",
            "Epoch 1 Batch 2000 Loss 1.2972\n",
            "Epoch 1 Batch 2050 Loss 1.3141\n",
            "Epoch 1 Batch 2100 Loss 1.3124\n",
            "Epoch 1 Batch 2150 Loss 1.3419\n",
            "Epoch 1 Batch 2200 Loss 1.3291\n",
            "Epoch 1 Batch 2250 Loss 1.2847\n",
            "Epoch 1 Batch 2300 Loss 1.3101\n",
            "Epoch 1 Batch 2350 Loss 1.2792\n",
            "Epoch 1 Batch 2400 Loss 1.3398\n",
            "Epoch 1 Batch 2450 Loss 1.2858\n",
            "Epoch 1 Batch 2500 Loss 1.2773\n",
            "Epoch 1 Batch 2550 Loss 1.3031\n",
            "Epoch 1 Batch 2600 Loss 1.3111\n",
            "Epoch 1 Batch 2650 Loss 1.3245\n",
            "Epoch 1 Batch 2700 Loss 1.2727\n",
            "Epoch 1 Batch 2750 Loss 1.3239\n",
            "Epoch 1 Batch 2800 Loss 1.2871\n",
            "Epoch 1 Batch 2850 Loss 1.2511\n",
            "Epoch 1 Batch 2900 Loss 1.2926\n",
            "Epoch 1 Batch 2950 Loss 1.2686\n",
            "Epoch 1 Batch 3000 Loss 1.2884\n",
            "Epoch 1 Batch 3050 Loss 1.3426\n",
            "Epoch 1 Batch 3100 Loss 1.2813\n",
            "Epoch 1 Batch 3150 Loss 1.3391\n",
            "Epoch 1 Batch 3200 Loss 1.2864\n",
            "Epoch 1 Batch 3250 Loss 1.2719\n",
            "Epoch 1 Batch 3300 Loss 1.3490\n",
            "Epoch 1 Batch 3350 Loss 1.2669\n",
            "Epoch 1 Batch 3400 Loss 1.2608\n",
            "Epoch 1 Batch 3450 Loss 1.2914\n",
            "Epoch 1 Batch 3500 Loss 1.3010\n",
            "Epoch 1 Batch 3550 Loss 1.2772\n",
            "\n",
            "Epoch 1 Loss: 1.3165\n",
            "Time taken for 1 epoch 142.27 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.3138\n",
            "Epoch 2 Batch 50 Loss 1.2985\n",
            "Epoch 2 Batch 100 Loss 1.2823\n",
            "Epoch 2 Batch 150 Loss 1.3415\n",
            "Epoch 2 Batch 200 Loss 1.2672\n",
            "Epoch 2 Batch 250 Loss 1.3166\n",
            "Epoch 2 Batch 300 Loss 1.3190\n",
            "Epoch 2 Batch 350 Loss 1.3372\n",
            "Epoch 2 Batch 400 Loss 1.3156\n",
            "Epoch 2 Batch 450 Loss 1.3110\n",
            "Epoch 2 Batch 500 Loss 1.2435\n",
            "Epoch 2 Batch 550 Loss 1.3290\n",
            "Epoch 2 Batch 600 Loss 1.3674\n",
            "Epoch 2 Batch 650 Loss 1.2742\n",
            "Epoch 2 Batch 700 Loss 1.3130\n",
            "Epoch 2 Batch 750 Loss 1.3186\n",
            "Epoch 2 Batch 800 Loss 1.3012\n",
            "Epoch 2 Batch 850 Loss 1.3041\n",
            "Epoch 2 Batch 900 Loss 1.2795\n",
            "Epoch 2 Batch 950 Loss 1.2788\n",
            "Epoch 2 Batch 1000 Loss 1.3286\n",
            "Epoch 2 Batch 1050 Loss 1.2753\n",
            "Epoch 2 Batch 1100 Loss 1.2663\n",
            "Epoch 2 Batch 1150 Loss 1.2984\n",
            "Epoch 2 Batch 1200 Loss 1.3324\n",
            "Epoch 2 Batch 1250 Loss 1.2866\n",
            "Epoch 2 Batch 1300 Loss 1.2631\n",
            "Epoch 2 Batch 1350 Loss 1.2922\n",
            "Epoch 2 Batch 1400 Loss 1.2291\n",
            "Epoch 2 Batch 1450 Loss 1.2580\n",
            "Epoch 2 Batch 1500 Loss 1.3019\n",
            "Epoch 2 Batch 1550 Loss 1.2256\n",
            "Epoch 2 Batch 1600 Loss 1.3111\n",
            "Epoch 2 Batch 1650 Loss 1.2619\n",
            "Epoch 2 Batch 1700 Loss 1.2800\n",
            "Epoch 2 Batch 1750 Loss 1.3062\n",
            "Epoch 2 Batch 1800 Loss 1.2603\n",
            "Epoch 2 Batch 1850 Loss 1.2500\n",
            "Epoch 2 Batch 1900 Loss 1.2577\n",
            "Epoch 2 Batch 1950 Loss 1.2514\n",
            "Epoch 2 Batch 2000 Loss 1.3170\n",
            "Epoch 2 Batch 2050 Loss 1.3099\n",
            "Epoch 2 Batch 2100 Loss 1.2652\n",
            "Epoch 2 Batch 2150 Loss 1.2445\n",
            "Epoch 2 Batch 2200 Loss 1.2571\n",
            "Epoch 2 Batch 2250 Loss 1.2376\n",
            "Epoch 2 Batch 2300 Loss 1.2825\n",
            "Epoch 2 Batch 2350 Loss 1.2584\n",
            "Epoch 2 Batch 2400 Loss 1.2501\n",
            "Epoch 2 Batch 2450 Loss 1.2839\n",
            "Epoch 2 Batch 2500 Loss 1.2543\n",
            "Epoch 2 Batch 2550 Loss 1.3128\n",
            "Epoch 2 Batch 2600 Loss 1.3017\n",
            "Epoch 2 Batch 2650 Loss 1.2729\n",
            "Epoch 2 Batch 2700 Loss 1.2838\n",
            "Epoch 2 Batch 2750 Loss 1.2597\n",
            "Epoch 2 Batch 2800 Loss 1.2990\n",
            "Epoch 2 Batch 2850 Loss 1.2826\n",
            "Epoch 2 Batch 2900 Loss 1.2807\n",
            "Epoch 2 Batch 2950 Loss 1.2992\n",
            "Epoch 2 Batch 3000 Loss 1.2721\n",
            "Epoch 2 Batch 3050 Loss 1.2774\n",
            "Epoch 2 Batch 3100 Loss 1.2896\n",
            "Epoch 2 Batch 3150 Loss 1.2895\n",
            "Epoch 2 Batch 3200 Loss 1.2812\n",
            "Epoch 2 Batch 3250 Loss 1.2622\n",
            "Epoch 2 Batch 3300 Loss 1.2881\n",
            "Epoch 2 Batch 3350 Loss 1.2674\n",
            "Epoch 2 Batch 3400 Loss 1.2286\n",
            "Epoch 2 Batch 3450 Loss 1.2513\n",
            "Epoch 2 Batch 3500 Loss 1.2915\n",
            "Epoch 2 Batch 3550 Loss 1.2678\n",
            "\n",
            "Epoch 2 Loss: 1.2760\n",
            "Time taken for 1 epoch 142.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.2733\n",
            "Epoch 3 Batch 50 Loss 1.2969\n",
            "Epoch 3 Batch 100 Loss 1.2571\n",
            "Epoch 3 Batch 150 Loss 1.2535\n",
            "Epoch 3 Batch 200 Loss 1.2220\n",
            "Epoch 3 Batch 250 Loss 1.3040\n",
            "Epoch 3 Batch 300 Loss 1.2415\n",
            "Epoch 3 Batch 350 Loss 1.2925\n",
            "Epoch 3 Batch 400 Loss 1.2638\n",
            "Epoch 3 Batch 450 Loss 1.2668\n",
            "Epoch 3 Batch 500 Loss 1.2719\n",
            "Epoch 3 Batch 550 Loss 1.2660\n",
            "Epoch 3 Batch 600 Loss 1.3026\n",
            "Epoch 3 Batch 650 Loss 1.2563\n",
            "Epoch 3 Batch 700 Loss 1.2663\n",
            "Epoch 3 Batch 750 Loss 1.2393\n",
            "Epoch 3 Batch 800 Loss 1.2331\n",
            "Epoch 3 Batch 850 Loss 1.2729\n",
            "Epoch 3 Batch 900 Loss 1.2711\n",
            "Epoch 3 Batch 950 Loss 1.2684\n",
            "Epoch 3 Batch 1000 Loss 1.2727\n",
            "Epoch 3 Batch 1050 Loss 1.2806\n",
            "Epoch 3 Batch 1100 Loss 1.2902\n",
            "Epoch 3 Batch 1150 Loss 1.2387\n",
            "Epoch 3 Batch 1200 Loss 1.2446\n",
            "Epoch 3 Batch 1250 Loss 1.2509\n",
            "Epoch 3 Batch 1300 Loss 1.2589\n",
            "Epoch 3 Batch 1350 Loss 1.2219\n",
            "Epoch 3 Batch 1400 Loss 1.2370\n",
            "Epoch 3 Batch 1450 Loss 1.2282\n",
            "Epoch 3 Batch 1500 Loss 1.2724\n",
            "Epoch 3 Batch 1550 Loss 1.2743\n",
            "Epoch 3 Batch 1600 Loss 1.2444\n",
            "Epoch 3 Batch 1650 Loss 1.2641\n",
            "Epoch 3 Batch 1700 Loss 1.2814\n",
            "Epoch 3 Batch 1750 Loss 1.2706\n",
            "Epoch 3 Batch 1800 Loss 1.2433\n",
            "Epoch 3 Batch 1850 Loss 1.2936\n",
            "Epoch 3 Batch 1900 Loss 1.2213\n",
            "Epoch 3 Batch 1950 Loss 1.2717\n",
            "Epoch 3 Batch 2000 Loss 1.2450\n",
            "Epoch 3 Batch 2050 Loss 1.2374\n",
            "Epoch 3 Batch 2100 Loss 1.2491\n",
            "Epoch 3 Batch 2150 Loss 1.2266\n",
            "Epoch 3 Batch 2200 Loss 1.2526\n",
            "Epoch 3 Batch 2250 Loss 1.2534\n",
            "Epoch 3 Batch 2300 Loss 1.2681\n",
            "Epoch 3 Batch 2350 Loss 1.2513\n",
            "Epoch 3 Batch 2400 Loss 1.2386\n",
            "Epoch 3 Batch 2450 Loss 1.2556\n",
            "Epoch 3 Batch 2500 Loss 1.2565\n",
            "Epoch 3 Batch 2550 Loss 1.2609\n",
            "Epoch 3 Batch 2600 Loss 1.2416\n",
            "Epoch 3 Batch 2650 Loss 1.2467\n",
            "Epoch 3 Batch 2700 Loss 1.2351\n",
            "Epoch 3 Batch 2750 Loss 1.2243\n",
            "Epoch 3 Batch 2800 Loss 1.2328\n",
            "Epoch 3 Batch 2850 Loss 1.2182\n",
            "Epoch 3 Batch 2900 Loss 1.2708\n",
            "Epoch 3 Batch 2950 Loss 1.2528\n",
            "Epoch 3 Batch 3000 Loss 1.2501\n",
            "Epoch 3 Batch 3050 Loss 1.2761\n",
            "Epoch 3 Batch 3100 Loss 1.2614\n",
            "Epoch 3 Batch 3150 Loss 1.2390\n",
            "Epoch 3 Batch 3200 Loss 1.2328\n",
            "Epoch 3 Batch 3250 Loss 1.2624\n",
            "Epoch 3 Batch 3300 Loss 1.2395\n",
            "Epoch 3 Batch 3350 Loss 1.2797\n",
            "Epoch 3 Batch 3400 Loss 1.2402\n",
            "Epoch 3 Batch 3450 Loss 1.2485\n",
            "Epoch 3 Batch 3500 Loss 1.2397\n",
            "Epoch 3 Batch 3550 Loss 1.2853\n",
            "\n",
            "Epoch 3 Loss: 1.2562\n",
            "Time taken for 1 epoch 141.25 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.2451\n",
            "Epoch 4 Batch 50 Loss 1.2810\n",
            "Epoch 4 Batch 100 Loss 1.2299\n",
            "Epoch 4 Batch 150 Loss 1.2771\n",
            "Epoch 4 Batch 200 Loss 1.2537\n",
            "Epoch 4 Batch 250 Loss 1.1883\n",
            "Epoch 4 Batch 300 Loss 1.2652\n",
            "Epoch 4 Batch 350 Loss 1.2322\n",
            "Epoch 4 Batch 400 Loss 1.2380\n",
            "Epoch 4 Batch 450 Loss 1.2637\n",
            "Epoch 4 Batch 500 Loss 1.2598\n",
            "Epoch 4 Batch 550 Loss 1.2390\n",
            "Epoch 4 Batch 600 Loss 1.2772\n",
            "Epoch 4 Batch 650 Loss 1.2333\n",
            "Epoch 4 Batch 700 Loss 1.2509\n",
            "Epoch 4 Batch 750 Loss 1.2583\n",
            "Epoch 4 Batch 800 Loss 1.2930\n",
            "Epoch 4 Batch 850 Loss 1.2323\n",
            "Epoch 4 Batch 900 Loss 1.2463\n",
            "Epoch 4 Batch 950 Loss 1.2339\n",
            "Epoch 4 Batch 1000 Loss 1.2209\n",
            "Epoch 4 Batch 1050 Loss 1.2393\n",
            "Epoch 4 Batch 1100 Loss 1.2519\n",
            "Epoch 4 Batch 1150 Loss 1.2274\n",
            "Epoch 4 Batch 1200 Loss 1.2557\n",
            "Epoch 4 Batch 1250 Loss 1.2938\n",
            "Epoch 4 Batch 1300 Loss 1.2494\n",
            "Epoch 4 Batch 1350 Loss 1.2632\n",
            "Epoch 4 Batch 1400 Loss 1.2687\n",
            "Epoch 4 Batch 1450 Loss 1.2789\n",
            "Epoch 4 Batch 1500 Loss 1.2815\n",
            "Epoch 4 Batch 1550 Loss 1.2455\n",
            "Epoch 4 Batch 1600 Loss 1.2596\n",
            "Epoch 4 Batch 1650 Loss 1.1878\n",
            "Epoch 4 Batch 1700 Loss 1.2377\n",
            "Epoch 4 Batch 1750 Loss 1.2437\n",
            "Epoch 4 Batch 1800 Loss 1.2494\n",
            "Epoch 4 Batch 1850 Loss 1.2283\n",
            "Epoch 4 Batch 1900 Loss 1.2126\n",
            "Epoch 4 Batch 1950 Loss 1.2921\n",
            "Epoch 4 Batch 2000 Loss 1.2508\n",
            "Epoch 4 Batch 2050 Loss 1.2449\n",
            "Epoch 4 Batch 2100 Loss 1.1954\n",
            "Epoch 4 Batch 2150 Loss 1.2539\n",
            "Epoch 4 Batch 2200 Loss 1.2406\n",
            "Epoch 4 Batch 2250 Loss 1.2021\n",
            "Epoch 4 Batch 2300 Loss 1.2446\n",
            "Epoch 4 Batch 2350 Loss 1.2302\n",
            "Epoch 4 Batch 2400 Loss 1.2403\n",
            "Epoch 4 Batch 2450 Loss 1.2033\n",
            "Epoch 4 Batch 2500 Loss 1.2582\n",
            "Epoch 4 Batch 2550 Loss 1.2083\n",
            "Epoch 4 Batch 2600 Loss 1.2510\n",
            "Epoch 4 Batch 2650 Loss 1.2411\n",
            "Epoch 4 Batch 2700 Loss 1.1957\n",
            "Epoch 4 Batch 2750 Loss 1.2506\n",
            "Epoch 4 Batch 2800 Loss 1.2091\n",
            "Epoch 4 Batch 2850 Loss 1.2855\n",
            "Epoch 4 Batch 2900 Loss 1.2068\n",
            "Epoch 4 Batch 2950 Loss 1.2658\n",
            "Epoch 4 Batch 3000 Loss 1.2924\n",
            "Epoch 4 Batch 3050 Loss 1.2534\n",
            "Epoch 4 Batch 3100 Loss 1.2403\n",
            "Epoch 4 Batch 3150 Loss 1.2545\n",
            "Epoch 4 Batch 3200 Loss 1.2376\n",
            "Epoch 4 Batch 3250 Loss 1.2612\n",
            "Epoch 4 Batch 3300 Loss 1.2259\n",
            "Epoch 4 Batch 3350 Loss 1.2713\n",
            "Epoch 4 Batch 3400 Loss 1.2365\n",
            "Epoch 4 Batch 3450 Loss 1.2571\n",
            "Epoch 4 Batch 3500 Loss 1.2332\n",
            "Epoch 4 Batch 3550 Loss 1.2484\n",
            "\n",
            "Epoch 4 Loss: 1.2452\n",
            "Time taken for 1 epoch 142.35 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.2231\n",
            "Epoch 5 Batch 50 Loss 1.2115\n",
            "Epoch 5 Batch 100 Loss 1.2142\n",
            "Epoch 5 Batch 150 Loss 1.2053\n",
            "Epoch 5 Batch 200 Loss 1.2274\n",
            "Epoch 5 Batch 250 Loss 1.2174\n",
            "Epoch 5 Batch 300 Loss 1.2173\n",
            "Epoch 5 Batch 350 Loss 1.2997\n",
            "Epoch 5 Batch 400 Loss 1.2862\n",
            "Epoch 5 Batch 450 Loss 1.2061\n",
            "Epoch 5 Batch 500 Loss 1.2276\n",
            "Epoch 5 Batch 550 Loss 1.3237\n",
            "Epoch 5 Batch 600 Loss 1.2406\n",
            "Epoch 5 Batch 650 Loss 1.2547\n",
            "Epoch 5 Batch 700 Loss 1.2335\n",
            "Epoch 5 Batch 750 Loss 1.1873\n",
            "Epoch 5 Batch 800 Loss 1.2389\n",
            "Epoch 5 Batch 850 Loss 1.1914\n",
            "Epoch 5 Batch 900 Loss 1.2209\n",
            "Epoch 5 Batch 950 Loss 1.2412\n",
            "Epoch 5 Batch 1000 Loss 1.2409\n",
            "Epoch 5 Batch 1050 Loss 1.2823\n",
            "Epoch 5 Batch 1100 Loss 1.2151\n",
            "Epoch 5 Batch 1150 Loss 1.2616\n",
            "Epoch 5 Batch 1200 Loss 1.2429\n",
            "Epoch 5 Batch 1250 Loss 1.2502\n",
            "Epoch 5 Batch 1300 Loss 1.2540\n",
            "Epoch 5 Batch 1350 Loss 1.2146\n",
            "Epoch 5 Batch 1400 Loss 1.2204\n",
            "Epoch 5 Batch 1450 Loss 1.2565\n",
            "Epoch 5 Batch 1500 Loss 1.2300\n",
            "Epoch 5 Batch 1550 Loss 1.2797\n",
            "Epoch 5 Batch 1600 Loss 1.2368\n",
            "Epoch 5 Batch 1650 Loss 1.2150\n",
            "Epoch 5 Batch 1700 Loss 1.2049\n",
            "Epoch 5 Batch 1750 Loss 1.2428\n",
            "Epoch 5 Batch 1800 Loss 1.2270\n",
            "Epoch 5 Batch 1850 Loss 1.2337\n",
            "Epoch 5 Batch 1900 Loss 1.2111\n",
            "Epoch 5 Batch 1950 Loss 1.2230\n",
            "Epoch 5 Batch 2000 Loss 1.2475\n",
            "Epoch 5 Batch 2050 Loss 1.2530\n",
            "Epoch 5 Batch 2100 Loss 1.2357\n",
            "Epoch 5 Batch 2150 Loss 1.2483\n",
            "Epoch 5 Batch 2200 Loss 1.2267\n",
            "Epoch 5 Batch 2250 Loss 1.2686\n",
            "Epoch 5 Batch 2300 Loss 1.2371\n",
            "Epoch 5 Batch 2350 Loss 1.2801\n",
            "Epoch 5 Batch 2400 Loss 1.2301\n",
            "Epoch 5 Batch 2450 Loss 1.2542\n",
            "Epoch 5 Batch 2500 Loss 1.2370\n",
            "Epoch 5 Batch 2550 Loss 1.2429\n",
            "Epoch 5 Batch 2600 Loss 1.2550\n",
            "Epoch 5 Batch 2650 Loss 1.1982\n",
            "Epoch 5 Batch 2700 Loss 1.2525\n",
            "Epoch 5 Batch 2750 Loss 1.2277\n",
            "Epoch 5 Batch 2800 Loss 1.2130\n",
            "Epoch 5 Batch 2850 Loss 1.2131\n",
            "Epoch 5 Batch 2900 Loss 1.2197\n",
            "Epoch 5 Batch 2950 Loss 1.2698\n",
            "Epoch 5 Batch 3000 Loss 1.2385\n",
            "Epoch 5 Batch 3050 Loss 1.2488\n",
            "Epoch 5 Batch 3100 Loss 1.2284\n",
            "Epoch 5 Batch 3150 Loss 1.2620\n",
            "Epoch 5 Batch 3200 Loss 1.2525\n",
            "Epoch 5 Batch 3250 Loss 1.2397\n",
            "Epoch 5 Batch 3300 Loss 1.2136\n",
            "Epoch 5 Batch 3350 Loss 1.2783\n",
            "Epoch 5 Batch 3400 Loss 1.2619\n",
            "Epoch 5 Batch 3450 Loss 1.2405\n",
            "Epoch 5 Batch 3500 Loss 1.2681\n",
            "Epoch 5 Batch 3550 Loss 1.2499\n",
            "\n",
            "Epoch 5 Loss: 1.2392\n",
            "Time taken for 1 epoch 142.50 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.2335\n",
            "Epoch 6 Batch 50 Loss 1.1992\n",
            "Epoch 6 Batch 100 Loss 1.1992\n",
            "Epoch 6 Batch 150 Loss 1.2437\n",
            "Epoch 6 Batch 200 Loss 1.2823\n",
            "Epoch 6 Batch 250 Loss 1.2345\n",
            "Epoch 6 Batch 300 Loss 1.2013\n",
            "Epoch 6 Batch 350 Loss 1.2441\n",
            "Epoch 6 Batch 400 Loss 1.2455\n",
            "Epoch 6 Batch 450 Loss 1.2072\n",
            "Epoch 6 Batch 500 Loss 1.2274\n",
            "Epoch 6 Batch 550 Loss 1.2465\n",
            "Epoch 6 Batch 600 Loss 1.2499\n",
            "Epoch 6 Batch 650 Loss 1.2060\n",
            "Epoch 6 Batch 700 Loss 1.2179\n",
            "Epoch 6 Batch 750 Loss 1.2273\n",
            "Epoch 6 Batch 800 Loss 1.2335\n",
            "Epoch 6 Batch 850 Loss 1.2120\n",
            "Epoch 6 Batch 900 Loss 1.2517\n",
            "Epoch 6 Batch 950 Loss 1.2314\n",
            "Epoch 6 Batch 1000 Loss 1.2301\n",
            "Epoch 6 Batch 1050 Loss 1.2439\n",
            "Epoch 6 Batch 1100 Loss 1.2055\n",
            "Epoch 6 Batch 1150 Loss 1.2628\n",
            "Epoch 6 Batch 1200 Loss 1.2264\n",
            "Epoch 6 Batch 1250 Loss 1.2131\n",
            "Epoch 6 Batch 1300 Loss 1.2237\n",
            "Epoch 6 Batch 1350 Loss 1.2614\n",
            "Epoch 6 Batch 1400 Loss 1.2718\n",
            "Epoch 6 Batch 1450 Loss 1.2438\n",
            "Epoch 6 Batch 1500 Loss 1.1610\n",
            "Epoch 6 Batch 1550 Loss 1.2721\n",
            "Epoch 6 Batch 1600 Loss 1.2190\n",
            "Epoch 6 Batch 1650 Loss 1.2380\n",
            "Epoch 6 Batch 1700 Loss 1.2658\n",
            "Epoch 6 Batch 1750 Loss 1.2619\n",
            "Epoch 6 Batch 1800 Loss 1.2025\n",
            "Epoch 6 Batch 1850 Loss 1.2615\n",
            "Epoch 6 Batch 1900 Loss 1.2487\n",
            "Epoch 6 Batch 1950 Loss 1.2496\n",
            "Epoch 6 Batch 2000 Loss 1.2204\n",
            "Epoch 6 Batch 2050 Loss 1.2076\n",
            "Epoch 6 Batch 2100 Loss 1.2096\n",
            "Epoch 6 Batch 2150 Loss 1.2633\n",
            "Epoch 6 Batch 2200 Loss 1.2408\n",
            "Epoch 6 Batch 2250 Loss 1.2403\n",
            "Epoch 6 Batch 2300 Loss 1.2180\n",
            "Epoch 6 Batch 2350 Loss 1.2362\n",
            "Epoch 6 Batch 2400 Loss 1.2106\n",
            "Epoch 6 Batch 2450 Loss 1.2283\n",
            "Epoch 6 Batch 2500 Loss 1.2584\n",
            "Epoch 6 Batch 2550 Loss 1.2290\n",
            "Epoch 6 Batch 2600 Loss 1.1921\n",
            "Epoch 6 Batch 2650 Loss 1.2325\n",
            "Epoch 6 Batch 2700 Loss 1.2059\n",
            "Epoch 6 Batch 2750 Loss 1.2355\n",
            "Epoch 6 Batch 2800 Loss 1.2395\n",
            "Epoch 6 Batch 2850 Loss 1.2222\n",
            "Epoch 6 Batch 2900 Loss 1.2266\n",
            "Epoch 6 Batch 2950 Loss 1.2349\n",
            "Epoch 6 Batch 3000 Loss 1.2489\n",
            "Epoch 6 Batch 3050 Loss 1.2748\n",
            "Epoch 6 Batch 3100 Loss 1.2376\n",
            "Epoch 6 Batch 3150 Loss 1.2003\n",
            "Epoch 6 Batch 3200 Loss 1.2186\n",
            "Epoch 6 Batch 3250 Loss 1.2182\n",
            "Epoch 6 Batch 3300 Loss 1.2351\n",
            "Epoch 6 Batch 3350 Loss 1.2520\n",
            "Epoch 6 Batch 3400 Loss 1.2460\n",
            "Epoch 6 Batch 3450 Loss 1.2808\n",
            "Epoch 6 Batch 3500 Loss 1.2452\n",
            "Epoch 6 Batch 3550 Loss 1.2310\n",
            "\n",
            "Epoch 6 Loss: 1.2360\n",
            "Time taken for 1 epoch 142.85 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2069\n",
            "Epoch 7 Batch 50 Loss 1.2331\n",
            "Epoch 7 Batch 100 Loss 1.2428\n",
            "Epoch 7 Batch 150 Loss 1.2195\n",
            "Epoch 7 Batch 200 Loss 1.2101\n",
            "Epoch 7 Batch 250 Loss 1.1925\n",
            "Epoch 7 Batch 300 Loss 1.2641\n",
            "Epoch 7 Batch 350 Loss 1.2496\n",
            "Epoch 7 Batch 400 Loss 1.2414\n",
            "Epoch 7 Batch 450 Loss 1.2608\n",
            "Epoch 7 Batch 500 Loss 1.1937\n",
            "Epoch 7 Batch 550 Loss 1.2602\n",
            "Epoch 7 Batch 600 Loss 1.2089\n",
            "Epoch 7 Batch 650 Loss 1.2043\n",
            "Epoch 7 Batch 700 Loss 1.2949\n",
            "Epoch 7 Batch 750 Loss 1.2420\n",
            "Epoch 7 Batch 800 Loss 1.2228\n",
            "Epoch 7 Batch 850 Loss 1.2541\n",
            "Epoch 7 Batch 900 Loss 1.2858\n",
            "Epoch 7 Batch 950 Loss 1.2226\n",
            "Epoch 7 Batch 1000 Loss 1.2195\n",
            "Epoch 7 Batch 1050 Loss 1.2554\n",
            "Epoch 7 Batch 1100 Loss 1.2341\n",
            "Epoch 7 Batch 1150 Loss 1.2419\n",
            "Epoch 7 Batch 1200 Loss 1.2330\n",
            "Epoch 7 Batch 1250 Loss 1.2504\n",
            "Epoch 7 Batch 1300 Loss 1.2273\n",
            "Epoch 7 Batch 1350 Loss 1.2668\n",
            "Epoch 7 Batch 1400 Loss 1.2319\n",
            "Epoch 7 Batch 1450 Loss 1.2624\n",
            "Epoch 7 Batch 1500 Loss 1.2354\n",
            "Epoch 7 Batch 1550 Loss 1.2369\n",
            "Epoch 7 Batch 1600 Loss 1.1962\n",
            "Epoch 7 Batch 1650 Loss 1.2523\n",
            "Epoch 7 Batch 1700 Loss 1.2364\n",
            "Epoch 7 Batch 1750 Loss 1.2480\n",
            "Epoch 7 Batch 1800 Loss 1.2245\n",
            "Epoch 7 Batch 1850 Loss 1.2197\n",
            "Epoch 7 Batch 1900 Loss 1.2343\n",
            "Epoch 7 Batch 1950 Loss 1.1822\n",
            "Epoch 7 Batch 2000 Loss 1.2478\n",
            "Epoch 7 Batch 2050 Loss 1.2202\n",
            "Epoch 7 Batch 2100 Loss 1.2397\n",
            "Epoch 7 Batch 2150 Loss 1.2427\n",
            "Epoch 7 Batch 2200 Loss 1.2499\n",
            "Epoch 7 Batch 2250 Loss 1.2645\n",
            "Epoch 7 Batch 2300 Loss 1.2390\n",
            "Epoch 7 Batch 2350 Loss 1.2898\n",
            "Epoch 7 Batch 2400 Loss 1.2483\n",
            "Epoch 7 Batch 2450 Loss 1.2254\n",
            "Epoch 7 Batch 2500 Loss 1.2265\n",
            "Epoch 7 Batch 2550 Loss 1.1978\n",
            "Epoch 7 Batch 2600 Loss 1.2407\n",
            "Epoch 7 Batch 2650 Loss 1.2399\n",
            "Epoch 7 Batch 2700 Loss 1.2077\n",
            "Epoch 7 Batch 2750 Loss 1.2428\n",
            "Epoch 7 Batch 2800 Loss 1.2222\n",
            "Epoch 7 Batch 2850 Loss 1.2440\n",
            "Epoch 7 Batch 2900 Loss 1.2040\n",
            "Epoch 7 Batch 2950 Loss 1.2127\n",
            "Epoch 7 Batch 3000 Loss 1.2626\n",
            "Epoch 7 Batch 3050 Loss 1.1803\n",
            "Epoch 7 Batch 3100 Loss 1.2397\n",
            "Epoch 7 Batch 3150 Loss 1.2410\n",
            "Epoch 7 Batch 3200 Loss 1.2515\n",
            "Epoch 7 Batch 3250 Loss 1.2035\n",
            "Epoch 7 Batch 3300 Loss 1.2295\n",
            "Epoch 7 Batch 3350 Loss 1.2085\n",
            "Epoch 7 Batch 3400 Loss 1.2192\n",
            "Epoch 7 Batch 3450 Loss 1.1969\n",
            "Epoch 7 Batch 3500 Loss 1.1609\n",
            "Epoch 7 Batch 3550 Loss 1.1764\n",
            "\n",
            "Epoch 7 Loss: 1.2349\n",
            "Time taken for 1 epoch 144.38 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2429\n",
            "Epoch 8 Batch 50 Loss 1.2608\n",
            "Epoch 8 Batch 100 Loss 1.1998\n",
            "Epoch 8 Batch 150 Loss 1.2188\n",
            "Epoch 8 Batch 200 Loss 1.2731\n",
            "Epoch 8 Batch 250 Loss 1.2089\n",
            "Epoch 8 Batch 300 Loss 1.2340\n",
            "Epoch 8 Batch 350 Loss 1.2670\n",
            "Epoch 8 Batch 400 Loss 1.2358\n",
            "Epoch 8 Batch 450 Loss 1.2110\n",
            "Epoch 8 Batch 500 Loss 1.2131\n",
            "Epoch 8 Batch 550 Loss 1.2278\n",
            "Epoch 8 Batch 600 Loss 1.2548\n",
            "Epoch 8 Batch 650 Loss 1.1921\n",
            "Epoch 8 Batch 700 Loss 1.2387\n",
            "Epoch 8 Batch 750 Loss 1.2283\n",
            "Epoch 8 Batch 800 Loss 1.2290\n",
            "Epoch 8 Batch 850 Loss 1.2249\n",
            "Epoch 8 Batch 900 Loss 1.2369\n",
            "Epoch 8 Batch 950 Loss 1.2718\n",
            "Epoch 8 Batch 1000 Loss 1.2111\n",
            "Epoch 8 Batch 1050 Loss 1.2448\n",
            "Epoch 8 Batch 1100 Loss 1.2704\n",
            "Epoch 8 Batch 1150 Loss 1.2323\n",
            "Epoch 8 Batch 1200 Loss 1.2894\n",
            "Epoch 8 Batch 1250 Loss 1.2350\n",
            "Epoch 8 Batch 1300 Loss 1.2283\n",
            "Epoch 8 Batch 1350 Loss 1.2519\n",
            "Epoch 8 Batch 1400 Loss 1.2456\n",
            "Epoch 8 Batch 1450 Loss 1.2457\n",
            "Epoch 8 Batch 1500 Loss 1.2305\n",
            "Epoch 8 Batch 1550 Loss 1.2342\n",
            "Epoch 8 Batch 1600 Loss 1.2756\n",
            "Epoch 8 Batch 1650 Loss 1.2422\n",
            "Epoch 8 Batch 1700 Loss 1.1930\n",
            "Epoch 8 Batch 1750 Loss 1.2595\n",
            "Epoch 8 Batch 1800 Loss 1.2876\n",
            "Epoch 8 Batch 1850 Loss 1.2410\n",
            "Epoch 8 Batch 1900 Loss 1.2378\n",
            "Epoch 8 Batch 1950 Loss 1.2355\n",
            "Epoch 8 Batch 2000 Loss 1.2502\n",
            "Epoch 8 Batch 2050 Loss 1.2238\n",
            "Epoch 8 Batch 2100 Loss 1.2577\n",
            "Epoch 8 Batch 2150 Loss 1.2484\n",
            "Epoch 8 Batch 2200 Loss 1.1692\n",
            "Epoch 8 Batch 2250 Loss 1.2200\n",
            "Epoch 8 Batch 2300 Loss 1.2387\n",
            "Epoch 8 Batch 2350 Loss 1.2110\n",
            "Epoch 8 Batch 2400 Loss 1.2313\n",
            "Epoch 8 Batch 2450 Loss 1.2291\n",
            "Epoch 8 Batch 2500 Loss 1.2838\n",
            "Epoch 8 Batch 2550 Loss 1.2470\n",
            "Epoch 8 Batch 2600 Loss 1.2804\n",
            "Epoch 8 Batch 2650 Loss 1.2452\n",
            "Epoch 8 Batch 2700 Loss 1.2437\n",
            "Epoch 8 Batch 2750 Loss 1.2225\n",
            "Epoch 8 Batch 2800 Loss 1.2047\n",
            "Epoch 8 Batch 2850 Loss 1.2548\n",
            "Epoch 8 Batch 2900 Loss 1.2422\n",
            "Epoch 8 Batch 2950 Loss 1.2393\n",
            "Epoch 8 Batch 3000 Loss 1.2020\n",
            "Epoch 8 Batch 3050 Loss 1.2231\n",
            "Epoch 8 Batch 3100 Loss 1.2874\n",
            "Epoch 8 Batch 3150 Loss 1.2518\n",
            "Epoch 8 Batch 3200 Loss 1.2360\n",
            "Epoch 8 Batch 3250 Loss 1.2646\n",
            "Epoch 8 Batch 3300 Loss 1.2077\n",
            "Epoch 8 Batch 3350 Loss 1.2135\n",
            "Epoch 8 Batch 3400 Loss 1.1982\n",
            "Epoch 8 Batch 3450 Loss 1.2397\n",
            "Epoch 8 Batch 3500 Loss 1.1853\n",
            "Epoch 8 Batch 3550 Loss 1.2262\n",
            "\n",
            "Epoch 8 Loss: 1.2351\n",
            "Time taken for 1 epoch 144.17 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.2116\n",
            "Epoch 9 Batch 50 Loss 1.2513\n",
            "Epoch 9 Batch 100 Loss 1.2355\n",
            "Epoch 9 Batch 150 Loss 1.2824\n",
            "Epoch 9 Batch 200 Loss 1.2445\n",
            "Epoch 9 Batch 250 Loss 1.2392\n",
            "Epoch 9 Batch 300 Loss 1.2264\n",
            "Epoch 9 Batch 350 Loss 1.2768\n",
            "Epoch 9 Batch 400 Loss 1.2512\n",
            "Epoch 9 Batch 450 Loss 1.2313\n",
            "Epoch 9 Batch 500 Loss 1.2721\n",
            "Epoch 9 Batch 550 Loss 1.2217\n",
            "Epoch 9 Batch 600 Loss 1.2189\n",
            "Epoch 9 Batch 650 Loss 1.2486\n",
            "Epoch 9 Batch 700 Loss 1.2570\n",
            "Epoch 9 Batch 750 Loss 1.2271\n",
            "Epoch 9 Batch 800 Loss 1.2257\n",
            "Epoch 9 Batch 850 Loss 1.2334\n",
            "Epoch 9 Batch 900 Loss 1.2782\n",
            "Epoch 9 Batch 950 Loss 1.2279\n",
            "Epoch 9 Batch 1000 Loss 1.2195\n",
            "Epoch 9 Batch 1050 Loss 1.2510\n",
            "Epoch 9 Batch 1100 Loss 1.2202\n",
            "Epoch 9 Batch 1150 Loss 1.2401\n",
            "Epoch 9 Batch 1200 Loss 1.2028\n",
            "Epoch 9 Batch 1250 Loss 1.2273\n",
            "Epoch 9 Batch 1300 Loss 1.2103\n",
            "Epoch 9 Batch 1350 Loss 1.2440\n",
            "Epoch 9 Batch 1400 Loss 1.2113\n",
            "Epoch 9 Batch 1450 Loss 1.2597\n",
            "Epoch 9 Batch 1500 Loss 1.2559\n",
            "Epoch 9 Batch 1550 Loss 1.2128\n",
            "Epoch 9 Batch 1600 Loss 1.2429\n",
            "Epoch 9 Batch 1650 Loss 1.2186\n",
            "Epoch 9 Batch 1700 Loss 1.2758\n",
            "Epoch 9 Batch 1750 Loss 1.2049\n",
            "Epoch 9 Batch 1800 Loss 1.2051\n",
            "Epoch 9 Batch 1850 Loss 1.2780\n",
            "Epoch 9 Batch 1900 Loss 1.2747\n",
            "Epoch 9 Batch 1950 Loss 1.2162\n",
            "Epoch 9 Batch 2000 Loss 1.2537\n",
            "Epoch 9 Batch 2050 Loss 1.2258\n",
            "Epoch 9 Batch 2100 Loss 1.2169\n",
            "Epoch 9 Batch 2150 Loss 1.2384\n",
            "Epoch 9 Batch 2200 Loss 1.2079\n",
            "Epoch 9 Batch 2250 Loss 1.2663\n",
            "Epoch 9 Batch 2300 Loss 1.2405\n",
            "Epoch 9 Batch 2350 Loss 1.2185\n",
            "Epoch 9 Batch 2400 Loss 1.2315\n",
            "Epoch 9 Batch 2450 Loss 1.2476\n",
            "Epoch 9 Batch 2500 Loss 1.2516\n",
            "Epoch 9 Batch 2550 Loss 1.1900\n",
            "Epoch 9 Batch 2600 Loss 1.2320\n",
            "Epoch 9 Batch 2650 Loss 1.2443\n",
            "Epoch 9 Batch 2700 Loss 1.2528\n",
            "Epoch 9 Batch 2750 Loss 1.2435\n",
            "Epoch 9 Batch 2800 Loss 1.2589\n",
            "Epoch 9 Batch 2850 Loss 1.2344\n",
            "Epoch 9 Batch 2900 Loss 1.2412\n",
            "Epoch 9 Batch 2950 Loss 1.2174\n",
            "Epoch 9 Batch 3000 Loss 1.2318\n",
            "Epoch 9 Batch 3050 Loss 1.2385\n",
            "Epoch 9 Batch 3100 Loss 1.2785\n",
            "Epoch 9 Batch 3150 Loss 1.2618\n",
            "Epoch 9 Batch 3200 Loss 1.1649\n",
            "Epoch 9 Batch 3250 Loss 1.2535\n",
            "Epoch 9 Batch 3300 Loss 1.2770\n",
            "Epoch 9 Batch 3350 Loss 1.2178\n",
            "Epoch 9 Batch 3400 Loss 1.2304\n",
            "Epoch 9 Batch 3450 Loss 1.2203\n",
            "Epoch 9 Batch 3500 Loss 1.1850\n",
            "Epoch 9 Batch 3550 Loss 1.2224\n",
            "\n",
            "Epoch 9 Loss: 1.2370\n",
            "Time taken for 1 epoch 144.60 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.2155\n",
            "Epoch 10 Batch 50 Loss 1.1998\n",
            "Epoch 10 Batch 100 Loss 1.1941\n",
            "Epoch 10 Batch 150 Loss 1.2459\n",
            "Epoch 10 Batch 200 Loss 1.2733\n",
            "Epoch 10 Batch 250 Loss 1.2681\n",
            "Epoch 10 Batch 300 Loss 1.2258\n",
            "Epoch 10 Batch 350 Loss 1.2199\n",
            "Epoch 10 Batch 400 Loss 1.2444\n",
            "Epoch 10 Batch 450 Loss 1.2133\n",
            "Epoch 10 Batch 500 Loss 1.2538\n",
            "Epoch 10 Batch 550 Loss 1.2200\n",
            "Epoch 10 Batch 600 Loss 1.2829\n",
            "Epoch 10 Batch 650 Loss 1.2526\n",
            "Epoch 10 Batch 700 Loss 1.2513\n",
            "Epoch 10 Batch 750 Loss 1.2524\n",
            "Epoch 10 Batch 800 Loss 1.2205\n",
            "Epoch 10 Batch 850 Loss 1.3298\n",
            "Epoch 10 Batch 900 Loss 1.2226\n",
            "Epoch 10 Batch 950 Loss 1.2336\n",
            "Epoch 10 Batch 1000 Loss 1.2265\n",
            "Epoch 10 Batch 1050 Loss 1.2206\n",
            "Epoch 10 Batch 1100 Loss 1.2601\n",
            "Epoch 10 Batch 1150 Loss 1.2192\n",
            "Epoch 10 Batch 1200 Loss 1.2418\n",
            "Epoch 10 Batch 1250 Loss 1.2199\n",
            "Epoch 10 Batch 1300 Loss 1.2515\n",
            "Epoch 10 Batch 1350 Loss 1.2125\n",
            "Epoch 10 Batch 1400 Loss 1.2491\n",
            "Epoch 10 Batch 1450 Loss 1.2184\n",
            "Epoch 10 Batch 1500 Loss 1.2592\n",
            "Epoch 10 Batch 1550 Loss 1.2084\n",
            "Epoch 10 Batch 1600 Loss 1.2250\n",
            "Epoch 10 Batch 1650 Loss 1.2199\n",
            "Epoch 10 Batch 1700 Loss 1.2384\n",
            "Epoch 10 Batch 1750 Loss 1.2804\n",
            "Epoch 10 Batch 1800 Loss 1.1664\n",
            "Epoch 10 Batch 1850 Loss 1.2445\n",
            "Epoch 10 Batch 1900 Loss 1.2320\n",
            "Epoch 10 Batch 1950 Loss 1.2187\n",
            "Epoch 10 Batch 2000 Loss 1.2905\n",
            "Epoch 10 Batch 2050 Loss 1.2050\n",
            "Epoch 10 Batch 2100 Loss 1.2069\n",
            "Epoch 10 Batch 2150 Loss 1.2404\n",
            "Epoch 10 Batch 2200 Loss 1.2561\n",
            "Epoch 10 Batch 2250 Loss 1.2513\n",
            "Epoch 10 Batch 2300 Loss 1.2740\n",
            "Epoch 10 Batch 2350 Loss 1.2371\n",
            "Epoch 10 Batch 2400 Loss 1.2796\n",
            "Epoch 10 Batch 2450 Loss 1.2834\n",
            "Epoch 10 Batch 2500 Loss 1.2907\n",
            "Epoch 10 Batch 2550 Loss 1.2309\n",
            "Epoch 10 Batch 2600 Loss 1.2239\n",
            "Epoch 10 Batch 2650 Loss 1.2345\n",
            "Epoch 10 Batch 2700 Loss 1.2279\n",
            "Epoch 10 Batch 2750 Loss 1.2343\n",
            "Epoch 10 Batch 2800 Loss 1.2825\n",
            "Epoch 10 Batch 2850 Loss 1.2677\n",
            "Epoch 10 Batch 2900 Loss 1.2710\n",
            "Epoch 10 Batch 2950 Loss 1.2342\n",
            "Epoch 10 Batch 3000 Loss 1.2527\n",
            "Epoch 10 Batch 3050 Loss 1.2622\n",
            "Epoch 10 Batch 3100 Loss 1.2021\n",
            "Epoch 10 Batch 3150 Loss 1.2316\n",
            "Epoch 10 Batch 3200 Loss 1.2335\n",
            "Epoch 10 Batch 3250 Loss 1.2318\n",
            "Epoch 10 Batch 3300 Loss 1.2096\n",
            "Epoch 10 Batch 3350 Loss 1.2253\n",
            "Epoch 10 Batch 3400 Loss 1.2131\n",
            "Epoch 10 Batch 3450 Loss 1.2387\n",
            "Epoch 10 Batch 3500 Loss 1.2416\n",
            "Epoch 10 Batch 3550 Loss 1.2344\n",
            "\n",
            "Epoch 10 Loss: 1.2398\n",
            "Time taken for 1 epoch 145.36 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RNN_USnews_generation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}